model_name: hyena-small
tokenizer_name: gpt2
model_config:
  _name_: lm
  d_model: 256 
  d_inner: 1024 
  n_layer: 8
  vocab_size: 4
  embed_dropout: 0.0
  layer:
    _name_: hyena
    emb_dim: 33
    linear_mixer: False
    filter_order: 64 
    local_order: 3
    l_max: 400_000 # 524288
    modulate: False
    w: 14
  fused_mlp: False
  fused_dropout_add_ln: False
  residual_in_fp32: True
  checkpoint_mixer: True
  checkpoint_mlp: True
  pad_vocab_size_multiple: 8